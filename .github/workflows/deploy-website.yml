name: Deploy Website (reusable workflow)

on:
  workflow_call:
    inputs:
      environment:
        description: 'The target environment/workspace to be used (dev, stg, prod)'
        type: string
        default: 'dev'
      region:
        description: 'The region available for the environment in AWS format embedded as a string, if applicable, i.e.: "us-west-2"'
        type: string
        default: 'global'
      working-directory:
        description: 'The location containing the files to be synced to S3'
        type: string
        default: 'website'
      terraform-artifact-name:
        description: 'Name of the terraform output artifact to download'
        type: string
        default: 'terraform-output'

permissions:
  id-token: write
  contents: read

env:
  ENVNAME: ${{ inputs.environment }}
  REGION: ${{ inputs.region }}

jobs:
  deploy_website:
    name: Deploy Website (${{ inputs.environment }}_${{ inputs.region }})
    runs-on: ${{ vars.RUNNERS || 'ubuntu-latest' }}
    steps:
    - name: Get source code locally from this repo
      uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1

    - name: Download Terraform output artifact
      uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v7.0.0
      with:
        name: ${{ inputs.terraform-artifact-name }}-${{ github.run_id }}
        path: /tmp/terraform-output

    - name: Parse Terraform outputs and perform replacements
      id: process_outputs
      shell: bash
      run: |
        # Read terraform output JSON
        TF_OUTPUT_FILE="/tmp/terraform-output/tf-output.json"
        
        if [ ! -f "$TF_OUTPUT_FILE" ]; then
          echo "Error: Terraform output file not found at $TF_OUTPUT_FILE"
          exit 1
        fi
        
        echo "Processing Terraform outputs..."
        
        # Extract all output keys and values
        OUTPUT_KEYS=$(jq -r 'keys[]' "$TF_OUTPUT_FILE")
        
        # Create a temporary script for all replacements
        REPLACE_SCRIPT="/tmp/replace_outputs.sh"
        cat > "$REPLACE_SCRIPT" << 'EOFSCRIPT'
        #!/bin/bash
        set -e
        
        TF_OUTPUT_FILE="$1"
        WORKING_DIR="$2"
        
        # Function to perform sed replacement
        perform_replacement() {
          local key=$1
          local value=$2
          local pattern="__${key}__"
          
          echo "Replacing $pattern with $value"
          
          # Find all files in working directory (excluding .git and hidden files)
          find "$WORKING_DIR" -type f ! -path "*/.git/*" ! -path "*/.*" -print0 | while IFS= read -r -d '' file; do
            # Skip binary files
            if file "$file" | grep -q "text"; then
              # Perform in-place replacement
              sed -i "s|${pattern}|${value}|g" "$file"
            fi
          done
        }
        
        # Process each output
        while IFS= read -r key; do
          value=$(jq -r --arg key "$key" '.[$key].value' "$TF_OUTPUT_FILE")
          
          # Handle different value types
          if [ "$value" != "null" ] && [ -n "$value" ]; then
            perform_replacement "$key" "$value"
          fi
        done < <(jq -r 'keys[]' "$TF_OUTPUT_FILE")
        
        echo "All replacements completed successfully"
        EOFSCRIPT
        
        chmod +x "$REPLACE_SCRIPT"
        
        # Execute the replacement script
        "$REPLACE_SCRIPT" "$TF_OUTPUT_FILE" "${{ inputs.working-directory }}"
        
        # Extract specific outputs needed for AWS operations
        WEBSITE_BUCKET=$(jq -r '.website_bucket_name.value // empty' "$TF_OUTPUT_FILE")
        CLOUDFRONT_ID=$(jq -r '.cloudfront_distribution_id.value // empty' "$TF_OUTPUT_FILE")
        
        echo "website_bucket=$WEBSITE_BUCKET" >> $GITHUB_OUTPUT
        echo "cloudfront_id=$CLOUDFRONT_ID" >> $GITHUB_OUTPUT
        
        echo "Extracted outputs:"
        echo "  Website bucket: $WEBSITE_BUCKET"
        echo "  CloudFront distribution ID: $CLOUDFRONT_ID"

    - name: Define the name of the variables to be retrieved based on the target environment
      id: aws_env_ids
      shell: bash
      run: |
        echo "role=${ENVNAME^^}_ROLE_SECRET" | tee -a $GITHUB_OUTPUT
        echo "account=${ENVNAME^^}_AWS_ID" | tee -a $GITHUB_OUTPUT
    
    - name: Set branch names
      id: repo_details
      shell: bash
      run: |
        echo "repo-name=$(echo ${{ github.repository }} | cut -d "/" -f2)" | tee -a $GITHUB_OUTPUT
        echo "roleprefix=github-role-for-$(echo "${{ github.repository }}" | cut -d "/" -f2 | cut -c1-17)-GitHubActionsRole" | tee -a $GITHUB_OUTPUT

    - name: Retrieve the AWS IAM Role to be used
      id: aws_role
      shell: bash
      run: echo "arn=arn:aws:iam::${{ secrets[ steps.aws_env_ids.outputs.account ] }}:role/${{ steps.repo_details.outputs.roleprefix }}-${{ secrets[ steps.aws_env_ids.outputs.role ] }}" | tee -a $GITHUB_OUTPUT

    - name: AWS CLI Login
      uses: aws-actions/configure-aws-credentials@d4695650384537945e0b565ee73a714d361bcb04  # v5.5.1
      with:
        role-to-assume: ${{ steps.aws_role.outputs.arn }}
        aws-region: ${{ inputs.region == 'global' && 'eu-central-1' || inputs.region  }}
        role-duration-seconds: '3600'

    - name: Sync files to S3 and capture changed paths
      id: s3_sync
      shell: bash
      run: |
        BUCKET="${{ steps.process_outputs.outputs.website_bucket }}"
        
        if [ -z "$BUCKET" ]; then
          echo "Error: website_bucket_name not found in Terraform outputs"
          exit 1
        fi
        
        # Create a temporary file to store sync output
        SYNC_OUTPUT="/tmp/s3-sync-output.txt"
        
        echo "Syncing ${{ inputs.working-directory }}/ to s3://$BUCKET/"
        
        # Run S3 sync and capture output
        aws s3 sync "${{ inputs.working-directory }}/" "s3://$BUCKET/" \
          --delete \
          --cache-control "max-age=3600" \
          2>&1 | tee "$SYNC_OUTPUT"
        
        # Extract the paths that were uploaded/deleted
        # S3 sync outputs lines like: "upload: file.html to s3://bucket/file.html"
        # or "delete: s3://bucket/file.html"
        # We need to extract just the S3 paths
        CHANGED_PATHS=""
        
        if [ -f "$SYNC_OUTPUT" ]; then
          # Extract paths from upload/delete operations
          CHANGED_PATHS=$(grep -E "(upload:|delete:)" "$SYNC_OUTPUT" | sed -E 's/.*(upload:.*to s3:\/\/[^\/]+\/|delete: s3:\/\/[^\/]+\/)/\//' || true)
        fi
        
        # If no specific paths were found, default to invalidating everything
        if [ -z "$CHANGED_PATHS" ]; then
          echo "No specific paths detected, will invalidate all"
          echo "invalidate_all=true" >> $GITHUB_OUTPUT
        else
          echo "invalidate_all=false" >> $GITHUB_OUTPUT
          
          # Save paths to a file
          echo "$CHANGED_PATHS" > /tmp/changed-paths.txt
          
          echo "Changed paths:"
          cat /tmp/changed-paths.txt
        fi

    - name: Create CloudFront invalidation
      if: steps.process_outputs.outputs.cloudfront_id != ''
      shell: bash
      run: |
        DISTRIBUTION_ID="${{ steps.process_outputs.outputs.cloudfront_id }}"
        
        if [ -z "$DISTRIBUTION_ID" ]; then
          echo "Warning: CloudFront distribution ID not found, skipping invalidation"
          exit 0
        fi
        
        # Determine paths to invalidate
        if [ "${{ steps.s3_sync.outputs.invalidate_all }}" == "true" ] || [ ! -f /tmp/changed-paths.txt ]; then
          PATHS="/*"
          echo "Creating CloudFront invalidation for all paths: $PATHS"
        else
          # Read changed paths and format for CloudFront
          PATHS=$(cat /tmp/changed-paths.txt | tr '\n' ' ')
          
          # If paths is empty or just whitespace, invalidate all
          if [ -z "$(echo "$PATHS" | tr -d '[:space:]')" ]; then
            PATHS="/*"
          fi
          
          echo "Creating CloudFront invalidation for paths: $PATHS"
        fi
        
        # Create the invalidation
        aws cloudfront create-invalidation \
          --distribution-id "$DISTRIBUTION_ID" \
          --paths $PATHS
        
        echo "CloudFront invalidation created successfully"
